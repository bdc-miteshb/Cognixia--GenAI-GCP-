{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to GCP and Semantic Kernel\n",
        "\n",
        "### Google Cloud Platform (GCP)\n",
        "* Google Cloud Platform (GCP) is a suite of cloud computing services offered by Google, providing a range of infrastructure, platform, and serverless computing environments.\n",
        "* It includes services like Google Cloud Storage (used in this notebook for file management), Compute Engine, BigQuery, and AI/ML tools. GCP enables scalable, secure, and efficient data storage, processing, and analysis, making it ideal for building and deploying applications, managing data, and integrating AI capabilities.\n",
        "\n",
        "### Semantic Kernel\n",
        "* Semantic Kernel is an open-source SDK developed by Microsoft that simplifies the integration of AI models and services into applications. * It acts as an orchestration layer, allowing developers to combine AI capabilities (like large language models from OpenAI) with traditional programming logic. It supports plugins, agents, and workflows, enabling the creation of intelligent applications that can process natural language, automate tasks, and integrate with external services like GCP.\n",
        "\n",
        "### Semantic Kernel with GCP\n",
        "* This notebook demonstrates the integration of Semantic Kernel with Google Cloud Platform (GCP) to create a multi-agent system for file management, analysis, and reporting.\n",
        "* Below are the code cells and their outputs, as provided in the original notebook.\n",
        "\n",
        "## Cell 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "KfmqnkdREs6n"
      },
      "id": "KfmqnkdREs6n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edcd6fb1",
      "metadata": {
        "id": "edcd6fb1"
      },
      "outputs": [],
      "source": [
        "!pip install semantic-kernel openai google-cloud-storage python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Import Libraries"
      ],
      "metadata": {
        "id": "hvwJegtmFC2L"
      },
      "id": "hvwJegtmFC2L"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66a527ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66a527ce",
        "outputId": "f8df3b44-3996-4131-9d19-f058865a88cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import semantic_kernel as sk\n",
        "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
        "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
        "from google.cloud import storage\n",
        "import openai\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ All libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Set Up Environment and Initialize Semantic Kernel"
      ],
      "metadata": {
        "id": "vbP3UkniFFQj"
      },
      "id": "vbP3UkniFFQj"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "01f11da7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01f11da7",
        "outputId": "465857f4-1b60-4ca7-d2ca-4126c6d64c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Kernel ready\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"Your_Jason-File-Path\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Your_API_KEY\"\n",
        "\n",
        "# Create Semantic Kernel\n",
        "kernel = sk.Kernel()\n",
        "kernel.add_service(OpenAIChatCompletion(\n",
        "    ai_model_id=\"gpt-4o\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    service_id=\"gpt\"\n",
        "))\n",
        "\n",
        "print(\"‚úÖ Kernel ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Define Enhanced GCP Agent"
      ],
      "metadata": {
        "id": "Xa8YuLtrFHmc"
      },
      "id": "Xa8YuLtrFHmc"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e1440d84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1440d84",
        "outputId": "b620fd42-5367-4e18-9df5-b432b8457359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced GCP Agent added to Semantic Kernel\n"
          ]
        }
      ],
      "source": [
        "class EnhancedGCPAgent:\n",
        "    \"\"\"Enhanced GCP Agent with better file handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = storage.Client(project=\"Gen-ai\")\n",
        "        self.bucket_name = \"bucket_demo8\"\n",
        "        self.base_dir = Path(r\"Your_File_Upload_PAth\")\n",
        "\n",
        "    @kernel_function(description=\"List files in bucket and local directory\")\n",
        "    def list_files(self) -> str:\n",
        "        # List files in bucket\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        bucket_files = [blob.name for blob in bucket.list_blobs()]\n",
        "\n",
        "        # List files in local directory\n",
        "        local_files = []\n",
        "        if self.base_dir.exists():\n",
        "            local_files = [f.name for f in self.base_dir.iterdir() if f.is_file()]\n",
        "\n",
        "        return f\"Bucket files: {bucket_files}\\nLocal files: {local_files}\"\n",
        "\n",
        "    @kernel_function(description=\"Upload file to bucket with smart filename matching\")\n",
        "    def upload_file(self, filename: str) -> str:\n",
        "        \"\"\"Upload file with smart filename matching\"\"\"\n",
        "        try:\n",
        "            # Try multiple filename variations\n",
        "            possible_names = [\n",
        "                filename,\n",
        "                filename.replace('_', ' '),\n",
        "                filename.replace(' ', '_'),\n",
        "                filename.replace('eer2', 'eer_2'),\n",
        "                filename.replace('eer_2', 'eer2')\n",
        "            ]\n",
        "\n",
        "            file_path = None\n",
        "            actual_filename = None\n",
        "\n",
        "            # Find the actual file\n",
        "            for name in possible_names:\n",
        "                test_path = self.base_dir / name\n",
        "                if test_path.exists():\n",
        "                    file_path = test_path\n",
        "                    actual_filename = name\n",
        "                    break\n",
        "\n",
        "            if file_path is None:\n",
        "                # List available files for user\n",
        "                if self.base_dir.exists():\n",
        "                    available_files = [f.name for f in self.base_dir.iterdir() if f.is_file()]\n",
        "                    return f\"‚ùå File '{filename}' not found. Available files: {available_files}\"\n",
        "                else:\n",
        "                    return f\"‚ùå Directory '{self.base_dir}' does not exist\"\n",
        "\n",
        "            # Upload the file\n",
        "            bucket = self.client.bucket(self.bucket_name)\n",
        "            blob = bucket.blob(filename)  # Use requested filename for blob\n",
        "            blob.upload_from_filename(str(file_path))\n",
        "\n",
        "            return f\"‚úÖ Uploaded '{actual_filename}' as '{filename}' to {self.bucket_name}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Upload error: {str(e)}\"\n",
        "\n",
        "    @kernel_function(description=\"Get detailed file info\")\n",
        "    def get_file_info(self, filename: str) -> str:\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        blob = bucket.blob(filename)\n",
        "        if blob.exists():\n",
        "            blob.reload()  # Get latest metadata\n",
        "            return f\"File: {filename}\\nSize: {blob.size} bytes\\nCreated: {blob.time_created}\\nType: {blob.content_type}\"\n",
        "        return f\"File {filename} not found in bucket\"\n",
        "\n",
        "    @kernel_function(description=\"Check local files in directory\")\n",
        "    def check_local_files(self) -> str:\n",
        "        \"\"\"Check what files are available locally\"\"\"\n",
        "        if not self.base_dir.exists():\n",
        "            return f\"‚ùå Directory {self.base_dir} does not exist\"\n",
        "\n",
        "        files = []\n",
        "        for file_path in self.base_dir.iterdir():\n",
        "            if file_path.is_file():\n",
        "                size = file_path.stat().st_size\n",
        "                files.append(f\"{file_path.name} ({size} bytes)\")\n",
        "\n",
        "        if files:\n",
        "            return f\"Local files in {self.base_dir}:\\n\" + \"\\n\".join(files)\n",
        "        else:\n",
        "            return f\"No files found in {self.base_dir}\"\n",
        "\n",
        "# Add Enhanced GCP Agent to kernel\n",
        "gcp_agent = EnhancedGCPAgent()\n",
        "kernel.add_plugin(gcp_agent, plugin_name=\"gcp\")\n",
        "print(\"‚úÖ Enhanced GCP Agent added to Semantic Kernel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Define Analysis Agent"
      ],
      "metadata": {
        "id": "oU7ZP5nbFKL6"
      },
      "id": "oU7ZP5nbFKL6"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f3d9e8a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3d9e8a7",
        "outputId": "93804df9-efd6-4f56-8222-bb8093949fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Analysis Agent created\n"
          ]
        }
      ],
      "source": [
        "class AnalysisAgent:\n",
        "    \"\"\"Analysis Agent using OpenAI\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    def analyze_files(self, file_data):\n",
        "        \"\"\"Analyze files\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a Data Analysis Agent. Analyze these files:\n",
        "\n",
        "        {file_data}\n",
        "\n",
        "        Provide:\n",
        "        1. File types found\n",
        "        2. Recommendations\n",
        "        3. Next steps\n",
        "\n",
        "        Keep it short.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "analysis_agent = AnalysisAgent()\n",
        "print(\"‚úÖ Analysis Agent created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6: Define Report Agent"
      ],
      "metadata": {
        "id": "-7q8JcgsFMc6"
      },
      "id": "-7q8JcgsFMc6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4eb71c86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eb71c86",
        "outputId": "acfa300d-4a15-4653-df01-75f11e19eddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Report Agent created\n"
          ]
        }
      ],
      "source": [
        "class ReportAgent:\n",
        "    \"\"\"Report Agent using OpenAI\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    def create_report(self, data):\n",
        "        \"\"\"Create report\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a Report Agent. Create a brief summary:\n",
        "\n",
        "        {data}\n",
        "\n",
        "        Provide:\n",
        "        1. Summary\n",
        "        2. Key findings\n",
        "        3. Recommended actions\n",
        "\n",
        "        Keep it concise.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "report_agent = ReportAgent()\n",
        "print(\"‚úÖ Report Agent created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 7: Define Multi-Agent Coordinator"
      ],
      "metadata": {
        "id": "1U_OD7BLFOcq"
      },
      "id": "1U_OD7BLFOcq"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "893b9f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "893b9f23",
        "outputId": "fae52557-1c56-4ff1-945f-b17ec20a16aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-Agent Coordinator ready\n"
          ]
        }
      ],
      "source": [
        "class MultiAgentCoordinator:\n",
        "    \"\"\"Coordinates all agents using Semantic Kernel\"\"\"\n",
        "\n",
        "    def __init__(self, kernel):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    async def run_workflow(self):\n",
        "        \"\"\"Run multi-agent workflow\"\"\"\n",
        "        print(\"üöÄ Multi-Agent Workflow Starting...\")\n",
        "\n",
        "        # Step 1: GCP Agent gets files (REAL Semantic Kernel)\n",
        "        print(\"\\nüìã GCP Agent: Getting files...\")\n",
        "        files = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "        print(f\"GCP Result: {files}\")\n",
        "\n",
        "        # Step 2: Analysis Agent analyzes\n",
        "        print(\"\\nüß† Analysis Agent: Analyzing...\")\n",
        "        analysis = analysis_agent.analyze_files(str(files))\n",
        "        print(f\"Analysis: {analysis[:100]}...\")\n",
        "\n",
        "        # Step 3: Report Agent creates report\n",
        "        print(\"\\nüìä Report Agent: Creating report...\")\n",
        "        report = report_agent.create_report(f\"Files: {files}\\nAnalysis: {analysis}\")\n",
        "        print(f\"Report: {report[:100]}...\")\n",
        "\n",
        "        return {\n",
        "            \"files\": str(files),\n",
        "            \"analysis\": analysis,\n",
        "            \"report\": report\n",
        "        }\n",
        "\n",
        "    async def upload_and_analyze(self, filename):\n",
        "        \"\"\"Upload and analyze using Semantic Kernel\"\"\"\n",
        "        print(f\"üì§ Upload and Analyze: {filename}\")\n",
        "\n",
        "        # Check local files first\n",
        "        local_check = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"check_local_files\"])\n",
        "        print(f\"Local files check: {local_check}\")\n",
        "\n",
        "        # Upload using REAL Semantic Kernel\n",
        "        upload_result = await self.kernel.invoke(\n",
        "            self.kernel.plugins[\"gcp\"][\"upload_file\"],\n",
        "            filename=filename\n",
        "        )\n",
        "        print(f\"Upload: {upload_result}\")\n",
        "\n",
        "        # Get files using REAL Semantic Kernel\n",
        "        files = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "\n",
        "        # Analyze with AI agent\n",
        "        analysis = analysis_agent.analyze_files(str(files))\n",
        "\n",
        "        return {\"upload\": str(upload_result), \"analysis\": analysis}\n",
        "\n",
        "# Create coordinator\n",
        "coordinator = MultiAgentCoordinator(kernel)\n",
        "print(\"‚úÖ Multi-Agent Coordinator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 8: Define Quick Commands"
      ],
      "metadata": {
        "id": "MuTAIwUjFRj6"
      },
      "id": "MuTAIwUjFRj6"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "05c11f5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05c11f5e",
        "outputId": "f2b745a6-3bc2-4323-b2a5-ca05824321f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced quick commands ready:\n",
            "- await quick_check_local()          # Check local files\n",
            "- await quick_list()                 # List bucket & local files\n",
            "- await quick_upload('filename')     # Smart upload\n",
            "- await quick_analyze()              # Multi-agent analysis\n",
            "- await quick_workflow()             # Full workflow\n",
            "- await quick_upload_and_analyze('filename')  # Upload + analyze\n"
          ]
        }
      ],
      "source": [
        "async def quick_list():\n",
        "    \"\"\"Quick file list using REAL Semantic Kernel\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "    print(files)\n",
        "    return files\n",
        "\n",
        "async def quick_check_local():\n",
        "    \"\"\"Check local files using REAL Semantic Kernel\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"check_local_files\"])\n",
        "    print(files)\n",
        "    return files\n",
        "\n",
        "async def quick_upload(filename):\n",
        "    \"\"\"Quick upload using REAL Semantic Kernel\"\"\"\n",
        "    result = await kernel.invoke(kernel.plugins[\"gcp\"][\"upload_file\"], filename=filename)\n",
        "    print(result)\n",
        "    return result\n",
        "\n",
        "async def quick_analyze():\n",
        "    \"\"\"Quick analysis using multi-agents\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "    analysis = analysis_agent.analyze_files(str(files))\n",
        "    print(analysis)\n",
        "    return analysis\n",
        "\n",
        "async def quick_workflow():\n",
        "    \"\"\"Quick full workflow\"\"\"\n",
        "    return await coordinator.run_workflow()\n",
        "\n",
        "async def quick_upload_and_analyze(filename):\n",
        "    \"\"\"Quick upload and analyze\"\"\"\n",
        "    return await coordinator.upload_and_analyze(filename)\n",
        "\n",
        "print(\"‚úÖ Enhanced quick commands ready:\")\n",
        "print(\"- await quick_check_local()          # Check local files\")\n",
        "print(\"- await quick_list()                 # List bucket & local files\")\n",
        "print(\"- await quick_upload('filename')     # Smart upload\")\n",
        "print(\"- await quick_analyze()              # Multi-agent analysis\")\n",
        "print(\"- await quick_workflow()             # Full workflow\")\n",
        "print(\"- await quick_upload_and_analyze('filename')  # Upload + analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 9: Test Enhanced System"
      ],
      "metadata": {
        "id": "h7u3Jt-yFT3y"
      },
      "id": "h7u3Jt-yFT3y"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "255b9d68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "255b9d68",
        "outputId": "32b5c502-3562-4084-c415-35b8255e18b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Enhanced System ===\n",
            "üîß 1. Checking local files...\n",
            "Local files in /content:\n",
            "temp_eer_2.pdf (1229101 bytes)\n",
            "gen-ai-462005-7ec6dfae325a.json (2357 bytes)\n",
            "\n",
            "üìã 2. Listing bucket and local files...\n",
            "Bucket files: ['main.py']\n",
            "Local files: ['temp_eer_2.pdf', 'gen-ai-462005-7ec6dfae325a.json']\n",
            "\n",
            "üì§ 3. Testing upload with temp_eer_2.pdf...\n",
            "‚úÖ Uploaded 'temp_eer_2.pdf' as 'temp_eer_2.pdf' to bucket_demo8\n",
            "\n",
            "üß† 4. Testing analysis...\n",
            "1. **File Types Found:**\n",
            "   - Python script: `main.py`\n",
            "   - PDF document: `temp_eer_2.pdf`\n",
            "   - JSON file: `gen-ai-462005-7ec6dfae325a.json`\n",
            "\n",
            "2. **Recommendations:**\n",
            "   - **For `main.py`:** Ensure the script is properly version-controlled and has clear comments/documentation for ease of maintenance.\n",
            "   - **For `temp_eer_2.pdf`:** If this is a temporary or generated file, consider organizing or archiving it if it‚Äôs not frequently used, to reduce clutter.\n",
            "   - **For `gen-ai-462005-7ec6dfae325a.json`:** Secure sensitive data within the JSON, especially if it contains API keys or personal information, and ensure compliance with data protection policies.\n",
            "\n",
            "3. **Next Steps:**\n",
            "   - Review and document the purpose and contents of each file.\n",
            "   - Implement version control for the Python script with a system like Git.\n",
            "   - Conduct a security audit on the JSON file contents and apply necessary encryption or access controls.\n",
            "   - Evaluate the need for storage optimization by regularly cleaning up or archiving temporary files.\n",
            "\n",
            "üöÄ 5. Testing full workflow...\n",
            "üöÄ Multi-Agent Workflow Starting...\n",
            "\n",
            "üìã GCP Agent: Getting files...\n",
            "GCP Result: Bucket files: ['main.py', 'temp_eer_2.pdf']\n",
            "Local files: ['temp_eer_2.pdf', 'gen-ai-462005-7ec6dfae325a.json']\n",
            "\n",
            "üß† Analysis Agent: Analyzing...\n",
            "Analysis: 1. **File Types Found:**\n",
            "   - Python script (`.py`)\n",
            "   - PDF document (`.pdf`)\n",
            "   - JSON file (`.jso...\n",
            "\n",
            "üìä Report Agent: Creating report...\n",
            "Report: **Summary:**\n",
            "The file set includes a Python script, a PDF document, and a JSON file, covering a rang...\n",
            "\n",
            "üéâ ALL TESTS COMPLETE!\n",
            "\n",
            "üöÄ Your Multi-Agent System is Ready! Use the commands above to interact with it.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Testing Enhanced System ===\")\n",
        "\n",
        "async def test_enhanced_system():\n",
        "    \"\"\"Test the enhanced system\"\"\"\n",
        "\n",
        "    print(\"üîß 1. Checking local files...\")\n",
        "    local_files = await quick_check_local()\n",
        "\n",
        "    print(\"\\nüìã 2. Listing bucket and local files...\")\n",
        "    all_files = await quick_list()\n",
        "\n",
        "    print(\"\\nüì§ 3. Testing upload with temp_eer_2.pdf...\")\n",
        "    upload_result = await quick_upload(\"temp_eer_2.pdf\")\n",
        "\n",
        "    print(\"\\nüß† 4. Testing analysis...\")\n",
        "    analysis = await quick_analyze()\n",
        "\n",
        "    print(\"\\nüöÄ 5. Testing full workflow...\")\n",
        "    workflow = await quick_workflow()\n",
        "\n",
        "    print(\"\\nüéâ ALL TESTS COMPLETE!\")\n",
        "    print(\"\\nüöÄ Your Multi-Agent System is Ready! Use the commands above to interact with it.\")\n",
        "\n",
        "await test_enhanced_system()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395d911b",
      "metadata": {
        "id": "395d911b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}