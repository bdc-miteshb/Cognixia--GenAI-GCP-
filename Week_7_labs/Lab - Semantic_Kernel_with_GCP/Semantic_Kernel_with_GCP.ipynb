{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "KfmqnkdREs6n",
      "metadata": {
        "id": "KfmqnkdREs6n"
      },
      "source": [
        "# Introduction to GCP and Semantic Kernel\n",
        "\n",
        "### Google Cloud Platform (GCP)\n",
        "* Google Cloud Platform (GCP) is a suite of cloud computing services offered by Google, providing a range of infrastructure, platform, and serverless computing environments.\n",
        "* It includes services like Google Cloud Storage (used in this notebook for file management), Compute Engine, BigQuery, and AI/ML tools. GCP enables scalable, secure, and efficient data storage, processing, and analysis, making it ideal for building and deploying applications, managing data, and integrating AI capabilities.\n",
        "\n",
        "### Semantic Kernel\n",
        "* Semantic Kernel is an open-source SDK developed by Microsoft that simplifies the integration of AI models and services into applications. * It acts as an orchestration layer, allowing developers to combine AI capabilities (like large language models from Gemini) with traditional programming logic. It supports plugins, agents, and workflows, enabling the creation of intelligent applications that can process natural language, automate tasks, and integrate with external services like GCP.\n",
        "\n",
        "### Semantic Kernel with GCP\n",
        "* This notebook demonstrates the integration of Semantic Kernel with Google Cloud Platform (GCP) to create a multi-agent system for file management, analysis, and reporting.\n",
        "* Below are the code cells and their outputs, as provided in the original notebook.\n",
        "\n",
        "## Cell 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "edcd6fb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edcd6fb1",
        "outputId": "8b5e0ada-c3ca-401e-e0f5-78434fdbffe3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyppeteer 2.0.0 requires pyee<12.0.0,>=11.0.0, but you have pyee 13.0.0 which is incompatible.\n",
            "pyppeteer 2.0.0 requires websockets<11.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install semantic-kernel google-generativeai google-cloud-storage python-dotenv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hvwJegtmFC2L",
      "metadata": {
        "id": "hvwJegtmFC2L"
      },
      "source": [
        "## Cell 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "66a527ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66a527ce",
        "outputId": "595383e2-746f-4e32-f60b-b0392cc4acba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All libraries imported\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import semantic_kernel as sk\n",
        "import google.generativeai as genai\n",
        "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
        "from google.cloud import storage\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"✅ All libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vbP3UkniFFQj",
      "metadata": {
        "id": "vbP3UkniFFQj"
      },
      "source": [
        "## Cell 3: Set Up Environment and Initialize Semantic Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f11da7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "01f11da7",
        "outputId": "ce6463e4-09f6-44cc-82cd-d280da757690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Gemini 1.5 Flash connected: Connection successful!  How can I help you today?\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"your_iam_crediential_role_json_file\"\n",
        "\n",
        "# Gemini Setup\n",
        "GEMINI_API_KEY = \"your_gemini_appi_key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Test Gemini connection\n",
        "try:\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    test_response = model.generate_content(\"Hello, test connection\")\n",
        "    print(f\"✅ Gemini 1.5 Flash connected: {test_response.text[:50]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Gemini connection failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xa8YuLtrFHmc",
      "metadata": {
        "id": "Xa8YuLtrFHmc"
      },
      "source": [
        "## Cell 4: Define Enhanced GCP Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ubCw31iVAjiE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubCw31iVAjiE",
        "outputId": "8dd19410-9525-49ce-9f69-ce2fc1136e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Kernel ready with Gemini 1.5 Flash\n"
          ]
        }
      ],
      "source": [
        "class GeminiChatCompletion:\n",
        "    \"\"\"Custom Gemini Chat Completion for Semantic Kernel\"\"\"\n",
        "\n",
        "    def __init__(self, api_key, model_name=\"gemini-1.5-flash\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.service_id = \"gemini\"\n",
        "\n",
        "    async def complete_chat_async(self, messages, **kwargs):\n",
        "        \"\"\"Complete chat using Gemini\"\"\"\n",
        "        try:\n",
        "            # Convert messages to prompt\n",
        "            prompt_parts = []\n",
        "            for message in messages:\n",
        "                if hasattr(message, 'role') and hasattr(message, 'content'):\n",
        "                    role = message.role\n",
        "                    content = message.content\n",
        "                elif isinstance(message, dict):\n",
        "                    role = message.get('role', '')\n",
        "                    content = message.get('content', '')\n",
        "                else:\n",
        "                    content = str(message)\n",
        "                    role = 'user'\n",
        "\n",
        "                if role == 'system':\n",
        "                    prompt_parts.append(f\"System: {content}\")\n",
        "                elif role == 'user':\n",
        "                    prompt_parts.append(f\"User: {content}\")\n",
        "                elif role == 'assistant':\n",
        "                    prompt_parts.append(f\"Assistant: {content}\")\n",
        "                else:\n",
        "                    prompt_parts.append(content)\n",
        "\n",
        "            full_prompt = \"\\n\".join(prompt_parts)\n",
        "\n",
        "            # Generate response\n",
        "            response = self.model.generate_content(full_prompt)\n",
        "\n",
        "            # Return in expected format\n",
        "            class ChatMessage:\n",
        "                def __init__(self, content):\n",
        "                    self.content = content\n",
        "                    self.role = \"assistant\"\n",
        "\n",
        "            return ChatMessage(response.text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini completion error: {str(e)}\")\n",
        "            class ErrorMessage:\n",
        "                def __init__(self, error):\n",
        "                    self.content = f\"Error: {str(error)}\"\n",
        "                    self.role = \"assistant\"\n",
        "            return ErrorMessage(e)\n",
        "\n",
        "# Create Kernel with Gemini\n",
        "kernel = sk.Kernel()\n",
        "\n",
        "# Add Gemini service to kernel\n",
        "gemini_service = GeminiChatCompletion(GEMINI_API_KEY)\n",
        "kernel.add_service(gemini_service)\n",
        "\n",
        "print(\"✅ Kernel ready with Gemini 1.5 Flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e1440d84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1440d84",
        "outputId": "40d39986-dbe0-4a45-854b-6fd5c8d99ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced GCP Agent added to Semantic Kernel\n"
          ]
        }
      ],
      "source": [
        "class EnhancedGCPAgent:\n",
        "    \"\"\"Enhanced GCP Agent with better file handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = storage.Client(project=\"Gen-ai\")  # Your project name\n",
        "        self.bucket_name = \"bucket_demo8\"  # Your bucket name\n",
        "        self.base_dir = Path(\"/content\")  # Your file path\n",
        "\n",
        "    @kernel_function(description=\"List files in bucket and local directory\")\n",
        "    def list_files(self) -> str:\n",
        "        \"\"\"List files in bucket and local directory\"\"\"\n",
        "        try:\n",
        "            # List files in bucket\n",
        "            bucket = self.client.bucket(self.bucket_name)\n",
        "            bucket_files = [blob.name for blob in bucket.list_blobs()]\n",
        "\n",
        "            # List files in local directory\n",
        "            local_files = []\n",
        "            if self.base_dir.exists():\n",
        "                local_files = [f.name for f in self.base_dir.iterdir() if f.is_file()]\n",
        "\n",
        "            return f\"Bucket files: {bucket_files}\\\\nLocal files: {local_files}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error listing files: {str(e)}\"\n",
        "\n",
        "    @kernel_function(description=\"Upload file to bucket with smart filename matching\")\n",
        "    def upload_file(self, filename: str) -> str:\n",
        "        \"\"\"Upload file with smart filename matching\"\"\"\n",
        "        try:\n",
        "            # Try multiple filename variations\n",
        "            possible_names = [\n",
        "                filename,\n",
        "                filename.replace('_', ' '),\n",
        "                filename.replace(' ', '_'),\n",
        "                filename.replace('eer2', 'eer_2'),\n",
        "                filename.replace('eer_2', 'eer2')\n",
        "            ]\n",
        "\n",
        "            file_path = None\n",
        "            actual_filename = None\n",
        "\n",
        "            # Find the actual file\n",
        "            for name in possible_names:\n",
        "                test_path = self.base_dir / name\n",
        "                if test_path.exists():\n",
        "                    file_path = test_path\n",
        "                    actual_filename = name\n",
        "                    break\n",
        "\n",
        "            if file_path is None:\n",
        "                # List available files for user\n",
        "                if self.base_dir.exists():\n",
        "                    available_files = [f.name for f in self.base_dir.iterdir() if f.is_file()]\n",
        "                    return f\"❌ File '{filename}' not found. Available files: {available_files}\"\n",
        "                else:\n",
        "                    return f\"❌ Directory '{self.base_dir}' does not exist\"\n",
        "\n",
        "            # Upload the file\n",
        "            bucket = self.client.bucket(self.bucket_name)\n",
        "            blob = bucket.blob(filename)  # Use requested filename for blob\n",
        "            blob.upload_from_filename(str(file_path))\n",
        "\n",
        "            return f\"✅ Uploaded '{actual_filename}' as '{filename}' to {self.bucket_name}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Upload error: {str(e)}\"\n",
        "\n",
        "    @kernel_function(description=\"Get detailed file info\")\n",
        "    def get_file_info(self, filename: str) -> str:\n",
        "        \"\"\"Get detailed file information\"\"\"\n",
        "        try:\n",
        "            bucket = self.client.bucket(self.bucket_name)\n",
        "            blob = bucket.blob(filename)\n",
        "            if blob.exists():\n",
        "                blob.reload()  # Get latest metadata\n",
        "                return f\"File: {filename}\\\\nSize: {blob.size} bytes\\\\nCreated: {blob.time_created}\\\\nType: {blob.content_type}\"\n",
        "            return f\"File {filename} not found in bucket\"\n",
        "        except Exception as e:\n",
        "            return f\"Error getting file info: {str(e)}\"\n",
        "\n",
        "    @kernel_function(description=\"Check local files in directory\")\n",
        "    def check_local_files(self) -> str:\n",
        "        \"\"\"Check what files are available locally\"\"\"\n",
        "        try:\n",
        "            if not self.base_dir.exists():\n",
        "                return f\"❌ Directory {self.base_dir} does not exist\"\n",
        "\n",
        "            files = []\n",
        "            for file_path in self.base_dir.iterdir():\n",
        "                if file_path.is_file():\n",
        "                    size = file_path.stat().st_size\n",
        "                    files.append(f\"{file_path.name} ({size} bytes)\")\n",
        "\n",
        "            if files:\n",
        "                return f\"Local files in {self.base_dir}:\\\\n\" + \"\\\\n\".join(files)\n",
        "            else:\n",
        "                return f\"No files found in {self.base_dir}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error checking local files: {str(e)}\"\n",
        "\n",
        "# Add Enhanced GCP Agent to kernel\n",
        "gcp_agent = EnhancedGCPAgent()\n",
        "kernel.add_plugin(gcp_agent, plugin_name=\"gcp\")\n",
        "print(\"✅ Enhanced GCP Agent added to Semantic Kernel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oU7ZP5nbFKL6",
      "metadata": {
        "id": "oU7ZP5nbFKL6"
      },
      "source": [
        "## Cell 5: Define Analysis Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f3d9e8a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3d9e8a7",
        "outputId": "392dce53-7d32-416c-f4f5-07bb51d1a3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis Agent created with Gemini 1.5 Flash\n"
          ]
        }
      ],
      "source": [
        "class AnalysisAgent:\n",
        "    \"\"\"Analysis Agent using Gemini 1.5 Flash\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    def analyze_files(self, file_data):\n",
        "        \"\"\"Analyze files using Gemini\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            You are a Data Analysis Agent. Analyze these files:\n",
        "\n",
        "            {file_data}\n",
        "\n",
        "            Provide:\n",
        "            1. File types found\n",
        "            2. Recommendations\n",
        "            3. Next steps\n",
        "\n",
        "            Keep it short and actionable.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Analysis error: {str(e)}\"\n",
        "\n",
        "analysis_agent = AnalysisAgent()\n",
        "print(\"✅ Analysis Agent created with Gemini 1.5 Flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-7q8JcgsFMc6",
      "metadata": {
        "id": "-7q8JcgsFMc6"
      },
      "source": [
        "## Cell 6: Define Report Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4eb71c86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eb71c86",
        "outputId": "d32db0ca-f24b-4592-e53b-3d702703957f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Report Agent created with Gemini 1.5 Flash\n"
          ]
        }
      ],
      "source": [
        "class ReportAgent:\n",
        "    \"\"\"Report Agent using Gemini 1.5 Flash\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    def create_report(self, data):\n",
        "        \"\"\"Create report using Gemini\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            You are a Report Agent. Create a brief summary:\n",
        "\n",
        "            {data}\n",
        "\n",
        "            Provide:\n",
        "            1. Summary\n",
        "            2. Key findings\n",
        "            3. Recommended actions\n",
        "\n",
        "            Keep it concise and professional.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Report error: {str(e)}\"\n",
        "\n",
        "report_agent = ReportAgent()\n",
        "print(\"✅ Report Agent created with Gemini 1.5 Flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1U_OD7BLFOcq",
      "metadata": {
        "id": "1U_OD7BLFOcq"
      },
      "source": [
        "## Cell 7: Define Multi-Agent Coordinator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "893b9f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "893b9f23",
        "outputId": "ba553900-8170-4b69-e715-2d15bc047e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Multi-Agent Coordinator ready\n"
          ]
        }
      ],
      "source": [
        "class MultiAgentCoordinator:\n",
        "    \"\"\"Coordinates all agents using Semantic Kernel with Gemini\"\"\"\n",
        "\n",
        "    def __init__(self, kernel):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    async def run_workflow(self):\n",
        "        \"\"\"Run multi-agent workflow\"\"\"\n",
        "        print(\"🚀 Multi-Agent Workflow Starting...\")\n",
        "\n",
        "        # Step 1: GCP Agent gets files (REAL Semantic Kernel)\n",
        "        print(\"\\\\n📋 GCP Agent: Getting files...\")\n",
        "        files = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "        print(f\"GCP Result: {files}\")\n",
        "\n",
        "        # Step 2: Analysis Agent analyzes (using Gemini)\n",
        "        print(\"\\\\n🧠 Analysis Agent: Analyzing...\")\n",
        "        analysis = analysis_agent.analyze_files(str(files))\n",
        "        print(f\"Analysis: {analysis[:100]}...\")\n",
        "\n",
        "        # Step 3: Report Agent creates report (using Gemini)\n",
        "        print(\"\\\\n📊 Report Agent: Creating report...\")\n",
        "        report = report_agent.create_report(f\"Files: {files}\\\\nAnalysis: {analysis}\")\n",
        "        print(f\"Report: {report[:100]}...\")\n",
        "\n",
        "        return {\n",
        "            \"files\": str(files),\n",
        "            \"analysis\": analysis,\n",
        "            \"report\": report\n",
        "        }\n",
        "\n",
        "    async def upload_and_analyze(self, filename):\n",
        "        \"\"\"Upload and analyze using Semantic Kernel\"\"\"\n",
        "        print(f\"📤 Upload and Analyze: {filename}\")\n",
        "\n",
        "        # Check local files first\n",
        "        local_check = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"check_local_files\"])\n",
        "        print(f\"Local files check: {local_check}\")\n",
        "\n",
        "        # Upload using REAL Semantic Kernel\n",
        "        upload_result = await self.kernel.invoke(\n",
        "            self.kernel.plugins[\"gcp\"][\"upload_file\"],\n",
        "            filename=filename\n",
        "        )\n",
        "        print(f\"Upload: {upload_result}\")\n",
        "\n",
        "        # Get files using REAL Semantic Kernel\n",
        "        files = await self.kernel.invoke(self.kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "\n",
        "        # Analyze with Gemini agent\n",
        "        analysis = analysis_agent.analyze_files(str(files))\n",
        "\n",
        "        return {\"upload\": str(upload_result), \"analysis\": analysis}\n",
        "\n",
        "# Create coordinator\n",
        "coordinator = MultiAgentCoordinator(kernel)\n",
        "print(\"✅ Multi-Agent Coordinator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MuTAIwUjFRj6",
      "metadata": {
        "id": "MuTAIwUjFRj6"
      },
      "source": [
        "## Cell 8: Define Quick Commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "05c11f5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05c11f5e",
        "outputId": "000bf669-4bcb-4713-e000-159f2d221d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced quick commands ready:\n",
            "- await quick_check_local()          # Check local files\n",
            "- await quick_list()                 # List bucket & local files\n",
            "- await quick_upload('filename')     # Smart upload\n",
            "- await quick_analyze()              # Multi-agent analysis with Gemini\n",
            "- await quick_workflow()             # Full workflow\n",
            "- await quick_upload_and_analyze('filename')  # Upload + analyze\n"
          ]
        }
      ],
      "source": [
        "async def quick_list():\n",
        "    \"\"\"Quick file list using REAL Semantic Kernel\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "    print(files)\n",
        "    return files\n",
        "\n",
        "async def quick_check_local():\n",
        "    \"\"\"Check local files using REAL Semantic Kernel\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"check_local_files\"])\n",
        "    print(files)\n",
        "    return files\n",
        "\n",
        "async def quick_upload(filename):\n",
        "    \"\"\"Quick upload using REAL Semantic Kernel\"\"\"\n",
        "    result = await kernel.invoke(kernel.plugins[\"gcp\"][\"upload_file\"], filename=filename)\n",
        "    print(result)\n",
        "    return result\n",
        "\n",
        "async def quick_analyze():\n",
        "    \"\"\"Quick analysis using Gemini agents\"\"\"\n",
        "    files = await kernel.invoke(kernel.plugins[\"gcp\"][\"list_files\"])\n",
        "    analysis = analysis_agent.analyze_files(str(files))\n",
        "    print(analysis)\n",
        "    return analysis\n",
        "\n",
        "async def quick_workflow():\n",
        "    \"\"\"Quick full workflow\"\"\"\n",
        "    return await coordinator.run_workflow()\n",
        "\n",
        "async def quick_upload_and_analyze(filename):\n",
        "    \"\"\"Quick upload and analyze\"\"\"\n",
        "    return await coordinator.upload_and_analyze(filename)\n",
        "\n",
        "print(\"✅ Enhanced quick commands ready:\")\n",
        "print(\"- await quick_check_local()          # Check local files\")\n",
        "print(\"- await quick_list()                 # List bucket & local files\")\n",
        "print(\"- await quick_upload('filename')     # Smart upload\")\n",
        "print(\"- await quick_analyze()              # Multi-agent analysis with Gemini\")\n",
        "print(\"- await quick_workflow()             # Full workflow\")\n",
        "print(\"- await quick_upload_and_analyze('filename')  # Upload + analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h7u3Jt-yFT3y",
      "metadata": {
        "id": "h7u3Jt-yFT3y"
      },
      "source": [
        "## Cell 9: Test Enhanced System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "255b9d68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "255b9d68",
        "outputId": "e1847775-43d4-43cc-9eee-5e7f702ba03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n=== Testing Enhanced System with Gemini 1.5 Flash ===\n",
            "🔧 1. Checking local files...\n",
            "Local files in /content:\\ngen-ai-462005-7ec6dfae325a.json (2357 bytes)\\ntemp_eer_2.pdf (1229101 bytes)\n",
            "\\n📋 2. Listing bucket and local files...\n",
            "Bucket files: ['main.py']\\nLocal files: ['gen-ai-462005-7ec6dfae325a.json', 'temp_eer_2.pdf']\n",
            "\\n📤 3. Testing upload with temp_eer_2.pdf...\n",
            "✅ Uploaded 'temp_eer_2.pdf' as 'temp_eer_2.pdf' to bucket_demo8\n",
            "\\n🧠 4. Testing analysis with Gemini 1.5 Flash...\n",
            "**1. File Types Found:** Python script (.py), JSON (.json), PDF (.pdf).\n",
            "\n",
            "**2. Recommendations:** Investigate `main.py` to understand its function and relation to `gen-ai-462005-7ec6dfae325a.json`.  Determine if `temp_eer_2.pdf` is a duplicate across locations and consider removing the redundant copy.\n",
            "\n",
            "**3. Next Steps:**\n",
            "* Open and review `main.py`.\n",
            "* Inspect `gen-ai-462005-7ec6dfae325a.json` contents.\n",
            "* Compare the two `temp_eer_2.pdf` files for identical content.  Delete the redundant file if confirmed.\n",
            "\n",
            "\\n🚀 5. Testing full workflow with Gemini...\n",
            "🚀 Multi-Agent Workflow Starting...\n",
            "\\n📋 GCP Agent: Getting files...\n",
            "GCP Result: Bucket files: ['main.py', 'temp_eer_2.pdf']\\nLocal files: ['gen-ai-462005-7ec6dfae325a.json', 'temp_eer_2.pdf']\n",
            "\\n🧠 Analysis Agent: Analyzing...\n",
            "Analysis: **1. File Types Found:** Python script (.py), JSON data (.json), PDF document (.pdf).\n",
            "\n",
            "**2. Recommen...\n",
            "\\n📊 Report Agent: Creating report...\n",
            "Report: **1. Summary:**\n",
            "\n",
            "This report analyzes files found in local storage and a cloud bucket.  Key files in...\n",
            "\\n🎉 ALL TESTS COMPLETE!\n",
            "\\n🚀 Your Multi-Agent System with Gemini 1.5 Flash is Ready!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'local_files': FunctionResult(function=KernelFunctionMetadata(name='check_local_files', plugin_name='gcp', description='Check local files in directory', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), value='Local files in /content:\\\\ngen-ai-462005-7ec6dfae325a.json (2357 bytes)\\\\ntemp_eer_2.pdf (1229101 bytes)', rendered_prompt=None, metadata={'arguments': {}, 'used_arguments': {}}),\n",
              " 'all_files': FunctionResult(function=KernelFunctionMetadata(name='list_files', plugin_name='gcp', description='List files in bucket and local directory', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), value=\"Bucket files: ['main.py']\\\\nLocal files: ['gen-ai-462005-7ec6dfae325a.json', 'temp_eer_2.pdf']\", rendered_prompt=None, metadata={'arguments': {}, 'used_arguments': {}}),\n",
              " 'upload_result': FunctionResult(function=KernelFunctionMetadata(name='upload_file', plugin_name='gcp', description='Upload file to bucket with smart filename matching', parameters=[KernelParameterMetadata(name='filename', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), value=\"✅ Uploaded 'temp_eer_2.pdf' as 'temp_eer_2.pdf' to bucket_demo8\", rendered_prompt=None, metadata={'arguments': {'filename': 'temp_eer_2.pdf'}, 'used_arguments': {'filename': 'temp_eer_2.pdf'}}),\n",
              " 'analysis': '**1. File Types Found:** Python script (.py), JSON (.json), PDF (.pdf).\\n\\n**2. Recommendations:** Investigate `main.py` to understand its function and relation to `gen-ai-462005-7ec6dfae325a.json`.  Determine if `temp_eer_2.pdf` is a duplicate across locations and consider removing the redundant copy.\\n\\n**3. Next Steps:**\\n* Open and review `main.py`.\\n* Inspect `gen-ai-462005-7ec6dfae325a.json` contents.\\n* Compare the two `temp_eer_2.pdf` files for identical content.  Delete the redundant file if confirmed.\\n',\n",
              " 'workflow': {'files': \"Bucket files: ['main.py', 'temp_eer_2.pdf']\\\\nLocal files: ['gen-ai-462005-7ec6dfae325a.json', 'temp_eer_2.pdf']\",\n",
              "  'analysis': \"**1. File Types Found:** Python script (.py), JSON data (.json), PDF document (.pdf).\\n\\n**2. Recommendations:** Investigate the relationship between `temp_eer_2.pdf` in both local and bucket storage.  Determine if `gen-ai-462005-7ec6dfae325a.json` contains relevant data for analysis in `main.py`.\\n\\n**3. Next Steps:**\\n* Check file content and metadata of `temp_eer_2.pdf` for inconsistencies (size, modification date).\\n* Examine `gen-ai-462005-7ec6dfae325a.json` structure and content; determine if it's input or output for `main.py`.\\n* Run `main.py` (safely, in a controlled environment) to understand its functionality.\\n\",\n",
              "  'report': '**1. Summary:**\\n\\nThis report analyzes files found in local storage and a cloud bucket.  Key files include a Python script (`main.py`), a JSON data file (`gen-ai-462005-7ec6dfae325a.json`), and a PDF document (`temp_eer_2.pdf`) present in both locations.  Further investigation is needed to understand the relationship between these files and their purpose.\\n\\n**2. Key Findings:**\\n\\n* `temp_eer_2.pdf` exists in both local and bucket storage, suggesting potential redundancy or a data synchronization issue.\\n* The purpose of `gen-ai-462005-7ec6dfae325a.json` and its connection to `main.py` requires clarification.\\n\\n**3. Recommended Actions:**\\n\\n* Verify the integrity of `temp_eer_2.pdf` by comparing file metadata across both locations.\\n* Analyze `gen-ai-462005-7ec6dfae325a.json` to determine its structure and content, and its role in relation to `main.py`.\\n* Execute `main.py` in a secure sandbox environment to assess its functionality and input/output requirements.\\n'}}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\\\n=== Testing Enhanced System with Gemini 1.5 Flash ===\")\n",
        "\n",
        "async def test_enhanced_system():\n",
        "    \"\"\"Test the enhanced system\"\"\"\n",
        "\n",
        "    print(\"🔧 1. Checking local files...\")\n",
        "    local_files = await quick_check_local()\n",
        "\n",
        "    print(\"\\\\n📋 2. Listing bucket and local files...\")\n",
        "    all_files = await quick_list()\n",
        "\n",
        "    print(\"\\\\n📤 3. Testing upload with temp_eer_2.pdf...\")\n",
        "    upload_result = await quick_upload(\"temp_eer_2.pdf\")\n",
        "\n",
        "    print(\"\\\\n🧠 4. Testing analysis with Gemini 1.5 Flash...\")\n",
        "    analysis = await quick_analyze()\n",
        "\n",
        "    print(\"\\\\n🚀 5. Testing full workflow with Gemini...\")\n",
        "    workflow = await quick_workflow()\n",
        "\n",
        "    print(\"\\\\n🎉 ALL TESTS COMPLETE!\")\n",
        "    print(\"\\\\n🚀 Your Multi-Agent System with Gemini 1.5 Flash is Ready!\")\n",
        "    return {\n",
        "        \"local_files\": local_files,\n",
        "        \"all_files\": all_files,\n",
        "        \"upload_result\": upload_result,\n",
        "        \"analysis\": analysis,\n",
        "        \"workflow\": workflow\n",
        "    }\n",
        "\n",
        "# Run the test\n",
        "await test_enhanced_system()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
