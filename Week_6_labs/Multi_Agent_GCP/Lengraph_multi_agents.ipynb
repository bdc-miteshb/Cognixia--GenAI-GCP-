{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##  Project Overview: Multi-Agent Chat App with Gemini & LangGraph\n",
        "\n",
        "###  **What This Project Does**\n",
        "\n",
        "* Lets users **choose between two AI agents**:\n",
        "\n",
        "  * **Research Agent**: Answers with factual, detailed insights.\n",
        "  * **Analysis Agent**: Extracts trends, insights, and patterns.\n",
        "* Runs a **Streamlit web app** served through **ngrok** for public access.\n",
        "* Uses **Gemini 1.5 Flash** via `langchain_google_genai` to power both agents.\n",
        "* Handles agent logic flow using **LangGraph's StateGraph**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Technologies Used\n",
        "\n",
        "| Tech        | Purpose                                                |\n",
        "| ----------- | ------------------------------------------------------ |\n",
        "| `Streamlit` | Web UI framework for interactive app                   |\n",
        "| `ngrok`     | Makes local app publicly accessible                    |\n",
        "| `LangChain` | Chains models and prompts together                     |\n",
        "| `LangGraph` | Defines agent workflow as a **stateful graph**         |\n",
        "| `Gemini`    | Google’s powerful multimodal LLM via Generative AI API |\n",
        "\n",
        "---\n",
        "\n",
        "##  LangGraph\n",
        "\n",
        "**LangGraph** is a powerful framework for building **agentic AI systems** with dynamic control flow, enabling **stateful** and **multi-agent reasoning**.\n",
        "\n",
        "###  Key Concepts\n",
        "\n",
        "- **StateGraph Model**: A graph-based abstraction where nodes are functions and edges represent transitions based on output state.\n",
        "- **Composable Agents**: Each node can represent a different tool or LLM agent, creating complex workflows.\n",
        "- **Memory/State Persistence**: Supports long-term memory and shared state between nodes.\n",
        "- **Dynamic Execution Flow**: Uses logic-based conditions or returned values to determine the next node.\n",
        "- **Loops and Recursion**: Supports advanced control flows via `graph.add_conditional_edges`.\n",
        "\n",
        "###  Use Cases\n",
        "\n",
        "- Task automation  \n",
        "- Chatbots with multiple personalities/tools  \n",
        "- Retrieval-Augmented Generation (RAG) agents  \n",
        "- Multi-step decision-making agents  \n",
        "\n",
        "---\n",
        "\n",
        "##  Gemini (Google Generative AI)\n",
        "\n",
        "**Gemini** is Google's family of **multimodal foundation models**, designed for tasks involving text, code, image, and reasoning.\n",
        "\n",
        "###  Key Features\n",
        "\n",
        "- **Multimodal Capability**: Handles text, image, audio, and video inputs.\n",
        "- **Gemini 1.5 Flash / Pro Models**: Optimized for low-latency or high-reasoning power.\n",
        "- **Native Tool Use**: Can interact with tools/functions using structured function calling.\n",
        "- **High Token Limit**: Context windows of up to **1 million tokens** in Gemini 1.5 Pro.\n",
        "- **Google Integration**: Works seamlessly with PaLM API, Vertex AI, and Google Workspace tools.\n",
        "\n",
        "###  Use Cases\n",
        "\n",
        "- Chatbots and assistants  \n",
        "- RAG-based question answering systems  \n",
        "- Multimodal understanding (e.g., image + text reasoning)  \n",
        "- Software agents with real-world planning abilities  \n",
        "\n",
        "---\n",
        "\n",
        "##  Multi-State Agent Theory\n",
        "\n",
        "**Multi-State Agents** are intelligent systems capable of transitioning between internal **states** during execution based on tasks, goals, or data.\n",
        "\n",
        "###  Core Concepts\n",
        "\n",
        "- **Stateful Memory**: Maintains a current context or \"state\" such as goals or subtasks.\n",
        "- **Dynamic Behavior**: Agent behavior changes depending on the current state (e.g., asking vs. verifying).\n",
        "- **Graph-Driven Transitions**: State transitions are guided by rules, logic, or outputs (LangGraph-powered).\n",
        "- **Emergent Planning**: Agents can re-evaluate decisions and adapt paths mid-execution.\n",
        "\n",
        "###  Modular Node Roles\n",
        "\n",
        "- `Planner`: Plans the next steps or decomposes tasks  \n",
        "- `Retriever`: Pulls relevant information from memory or tools  \n",
        "- `Executor`: Carries out tasks or tool use  \n",
        "- `Verifier`: Validates outputs or decisions  \n",
        "\n",
        "---\n",
        "\n",
        "##  About the Agents\n",
        "\n",
        "### 1. Research Agent\n",
        "\n",
        "* Uses a Gemini model with **temperature = 0.7**\n",
        "* Designed for **open-ended, factual Q\\&A**\n",
        "* Prompt guides it to behave like a research assistant\n",
        "\n",
        "### 2. Analysis Agent\n",
        "\n",
        "* Uses Gemini with **temperature = 0.3** (less randomness)\n",
        "* Analyzes input for patterns, insights, summaries\n",
        "* Ideal for extracting meaning from text-heavy data\n",
        "\n",
        "---\n",
        "\n",
        "##  How It All Works\n",
        "\n",
        "1. User selects an agent (Research or Analysis) from the UI.\n",
        "2. Inputs a question.\n",
        "3. A **LangGraph** is dynamically built with the appropriate agent logic.\n",
        "4. Gemini generates a response based on the selected agent's behavior.\n",
        "5. The response is shown in the Streamlit app.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "974sM83w6ajk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 : Install required packages"
      ],
      "metadata": {
        "id": "F0OiYJ3bu4b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit langgraph langchain-google-genai langchain-core pyngrok nest-asyncio\n"
      ],
      "metadata": {
        "id": "7Hk-UzRIu2Wj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2 : Make agentic_app.py for creating agents.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gtKj7Ym9vfhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agentic_app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import nest_asyncio\n",
        "from typing import List, TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: List\n",
        "    output: str\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model(temp):\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key, temperature=temp)\n",
        "\n",
        "def research_agent(state: AgentState) -> AgentState:\n",
        "    model = load_model(temp=0.7)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful research agent. Provide detailed insights and factual info.\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    chain = prompt | model\n",
        "    user_input = state[\"messages\"][-1].content\n",
        "    response = chain.invoke({\"input\": user_input})\n",
        "    state[\"output\"] = response.content\n",
        "    return state\n",
        "\n",
        "def analysis_agent(state: AgentState) -> AgentState:\n",
        "    model = load_model(temp=0.3)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are an analysis agent. Extract insights, trends, and patterns from user input.\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    chain = prompt | model\n",
        "    user_input = state[\"messages\"][-1].content\n",
        "    response = chain.invoke({\"input\": user_input})\n",
        "    state[\"output\"] = response.content\n",
        "    return state\n",
        "\n",
        "@st.cache_resource\n",
        "def setup_graph(agent_type: str):\n",
        "    graph = StateGraph(AgentState)\n",
        "    if agent_type == \"Research Agent\":\n",
        "        graph.add_node(\"agent\", research_agent)\n",
        "    else:\n",
        "        graph.add_node(\"agent\", analysis_agent)\n",
        "    graph.set_entry_point(\"agent\")\n",
        "    graph.add_edge(\"agent\", END)\n",
        "    return graph.compile()\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Agent Selector\", layout=\"wide\")\n",
        "    st.title(\"Select Your AI Agent\")\n",
        "\n",
        "    if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "        api_key = st.sidebar.text_input(\"Enter Google Gemini API Key:\", type=\"password\")\n",
        "        if api_key:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "        else:\n",
        "            st.stop()\n",
        "\n",
        "    agent_type = st.radio(\"Choose Agent:\", [\"Research Agent\", \"Analysis Agent\"])\n",
        "\n",
        "    query = st.text_input(\"Enter your question:\")\n",
        "    run_btn = st.button(\"Run Agent\")\n",
        "\n",
        "    if run_btn and query:\n",
        "        graph = setup_graph(agent_type)\n",
        "        state = {\"messages\": [HumanMessage(content=query)], \"output\": \"\"}\n",
        "        result = graph.invoke(state)\n",
        "        st.markdown(f\"### Response from **{agent_type}**\")\n",
        "        st.markdown(result[\"output\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aDQDfBokveQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2861f965-10fa-486f-d199-4b7763ba083b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting agentic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 3 : Launch Streamlit"
      ],
      "metadata": {
        "id": "MArM-ORLv4Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#  Set your API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = input(\"🔑 Enter your Gemini API key: \")\n",
        "ngrok_token = input(\"🔑 Enter your ngrok auth token: \")\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "#  Close previous tunnels\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "\n",
        "#  Open tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\" App URL: {public_url}\")\n",
        "\n",
        "#  Launch Streamlit\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"agentic_app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Keep cell running\n",
        "while True:\n",
        "    time.sleep(60)\n"
      ],
      "metadata": {
        "id": "vEUwSmQPv1pc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "41ae2fad-fd10-4e81-bc5e-6c8a56fe97bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Enter your Gemini API key: AIzaSyBSBbhtj-XlYtLaG4TvOCHImIjUm0_EKAA\n",
            "🔑 Enter your ngrok auth token: 2jjXnp0rBA9TrOb3l9BiBl3HKNS_5a5jtyMEuxG1mWkX9gbFj\n",
            "🌍 App URL: NgrokTunnel: \"https://3b683302cd82.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3383474667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Keep cell running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}