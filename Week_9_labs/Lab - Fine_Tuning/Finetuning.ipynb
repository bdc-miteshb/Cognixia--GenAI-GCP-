{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "908ee553da4f4007b235e83c73bcda57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23823256d2744975ba751bcf9ac86625",
              "IPY_MODEL_2459bfb2163c416eb8088b22ce81fe34",
              "IPY_MODEL_ac616fc4f65e48ce8bf9cec073b4eb52"
            ],
            "layout": "IPY_MODEL_596fb7c1d4dd475da283334863c5abc0"
          }
        },
        "23823256d2744975ba751bcf9ac86625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c98bd06992a44abfaa593e2d78b72805",
            "placeholder": "​",
            "style": "IPY_MODEL_4daa76383a2b4965bbb9e2e329dc368f",
            "value": "Map: 100%"
          }
        },
        "2459bfb2163c416eb8088b22ce81fe34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb6daa2dc5344678c782602066d66f4",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96904e4f0e504fd78ec9fd1a4fe4c7fb",
            "value": 30
          }
        },
        "ac616fc4f65e48ce8bf9cec073b4eb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72c4d44d5b274fd087a44b4da4d56abb",
            "placeholder": "​",
            "style": "IPY_MODEL_6bee63ea34b54c89b707e09ad812de7f",
            "value": " 30/30 [00:00&lt;00:00, 65.48 examples/s]"
          }
        },
        "596fb7c1d4dd475da283334863c5abc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c98bd06992a44abfaa593e2d78b72805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4daa76383a2b4965bbb9e2e329dc368f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb6daa2dc5344678c782602066d66f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96904e4f0e504fd78ec9fd1a4fe4c7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72c4d44d5b274fd087a44b4da4d56abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bee63ea34b54c89b707e09ad812de7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  GPT Fine-Tuning using Hugging Face Transformers\n",
        "\n",
        "# Objective\n",
        "\n",
        "The objective of this project is to **fine-tune the GPT-2 language model** on a custom dataset (extracted from a PDF) using the Hugging Face Transformers library. By doing this, we aim to:\n",
        "\n",
        "- **Customize GPT-2’s knowledge** to a specific domain or document (e.g., a textbook, manual, or company document).\n",
        "- **Enable accurate and context-aware question answering** from the fine-tuned model.\n",
        "- **Build a foundation for a domain-specific chatbot or assistant** that understands the context of the PDF.\n",
        "- Learn and demonstrate how to implement **end-to-end fine-tuning using Hugging Face’s `Trainer` API**.\n",
        "\n",
        "This fine-tuned model can later be used for:\n",
        "- Chatbots,\n",
        "- QA systems,\n",
        "- Summarization tools,\n",
        "- Document-based assistants,\n",
        "with **better performance and relevance** than a generic GPT-2 model.\n",
        "\n",
        "\n",
        "## What is GPT?\n",
        "- **GPT (Generative Pretrained Transformer 2)** is an open-source language model developed by OpenAI.\n",
        "- It is based on the **Transformer decoder architecture**.\n",
        "- GPT is a **pretrained model**, which means it is trained on a massive amount of text data to understand and generate human-like text.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Fine-Tuning?\n",
        "- Fine-tuning is a process of **further training a pretrained model on a specific dataset**.\n",
        "- This adapts the general language understanding of the model to a **specific task** (e.g., answering questions from a PDF, chatbot, summarization).\n",
        "- You **don’t train from scratch**, but build on top of existing knowledge.\n",
        "\n",
        "### Benefits of Fine-Tuning:\n",
        "- Saves **computation time** and **resources**.\n",
        "- Achieves **higher accuracy** on domain-specific tasks.\n",
        "- Easy to implement using high-level APIs like Hugging Face’s `Trainer`.\n",
        "\n",
        "---\n",
        "\n",
        "## Hugging Face Transformers Library\n",
        "- Hugging Face `transformers` is a **popular open-source library** for working with NLP models like GPT, BERT, T5, etc.\n",
        "- It provides:\n",
        "  - Pretrained models via `AutoModel`, `GPT2LMHeadModel`, etc.\n",
        "  - Tokenizers\n",
        "  - Training utilities like `Trainer` and `TrainingArguments`.\n",
        "\n",
        "---\n",
        "\n",
        "## What is the Hugging Face `Trainer`?\n",
        "The `Trainer` class simplifies the process of training and fine-tuning models.\n",
        "\n",
        "### Key Features:\n",
        "- Handles **training loops**, **evaluation**, **saving checkpoints**, and **logging** automatically.\n",
        "- Supports **custom datasets** using `Dataset` or `datasets.load_dataset`.\n",
        "- Allows configuration using `TrainingArguments`.\n",
        "\n",
        "### Example Components Used in Trainer:\n",
        "- `model`: Your GPT-2 model (`GPT2LMHeadModel`).\n",
        "- `train_dataset`: Your dataset in tokenized format.\n",
        "- `tokenizer`: Tokenizer to convert text into input IDs.\n",
        "- `args`: TrainingArguments (e.g., learning rate, output directory, batch size).\n",
        "\n",
        "---\n",
        "\n",
        "## Training Process Summary\n",
        "\n",
        "### 1. **Dataset Preparation**\n",
        "- You created a dataset from a PDF file using PyPDF2.\n",
        "- Cleaned and chunked the text for fine-tuning.\n",
        "- Converted text into a custom Dataset class.\n",
        "\n",
        "### 2. **Tokenizer**\n",
        "- Used `GPT2Tokenizer` to tokenize the text data.\n",
        "- Added padding and truncation for consistency in input sizes.\n",
        "\n",
        "### 3. **Model Initialization**\n",
        "- Loaded `GPT2LMHeadModel` for language modeling.\n",
        "- Set to training mode using `.train()`.\n",
        "\n",
        "### 4. **Trainer Setup**\n",
        "- Defined `TrainingArguments`: epochs, batch size, logging, and checkpoint saving.\n",
        "- Initialized the `Trainer` with model, tokenizer, dataset, and arguments.\n",
        "- Called `trainer.train()` to begin fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## Saving and Loading Fine-Tuned Model\n",
        "- After training, model and tokenizer were saved using:\n",
        "  ```python\n",
        "  trainer.save_model(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "FhOUSR15sUiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 1: Install Libraries"
      ],
      "metadata": {
        "id": "4FasBHJh73VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate PyPDF2"
      ],
      "metadata": {
        "id": "JcqloPOHA1RX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 2: Imports"
      ],
      "metadata": {
        "id": "TbanlmgxA4yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "JCVk7BfuA6u_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 3: Prepare Dataset"
      ],
      "metadata": {
        "id": "Bsh5K68gqSOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Extract and clean the text\n",
        "raw_text = extract_text_from_pdf(\"/content/attention.pdf\")\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
        "\n",
        "# Preview\n",
        "print(cleaned_text[:1000])  # Print first 1000 characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sIs-tTMJ7b3",
        "outputId": "812fb601-36ef-4921-9ab0-ba43786280b7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Is All You Need Ashish Vaswani\u0003 Google Brain avaswani@google.comNoam Shazeer\u0003 Google Brain noam@google.comNiki Parmar\u0003 Google Research nikip@google.comJakob Uszkoreit\u0003 Google Research usz@google.com Llion Jones\u0003 Google Research llion@google.comAidan N. Gomez\u0003y University of Toronto aidan@cs.toronto.eduŁukasz Kaiser\u0003 Google Brain lukaszkaiser@google.com Illia Polosukhin\u0003z illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achiev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 4: Auto-create basic Q&A pairs from knowledge"
      ],
      "metadata": {
        "id": "Btey4UZAqW7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Auto-create basic Q&A pairs from knowledge\n",
        "sentences = re.split(r'(?<=[.?!])\\s+', cleaned_text)\n",
        "qa_data = []\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    if len(sentence) < 30:\n",
        "        continue\n",
        "    qa_data.append({\n",
        "        \"text\": f\"Question: What does the document say in point {i+1}?\\nAnswer: {sentence}\"\n",
        "    })\n",
        "    if len(qa_data) >= 30:  # Limit size for demo\n",
        "        break\n"
      ],
      "metadata": {
        "id": "wCVgvGUbJ8GY"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 5: Load GPT-2 and Tokenizer"
      ],
      "metadata": {
        "id": "XRRSY1cPqiMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load dataset\n",
        "dataset = Dataset.from_list(qa_data)\n",
        "\n",
        "# 4. Load tokenizer & model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad with EOS\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "PST0xeWGKgxI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 6: Tokenize the Dataset"
      ],
      "metadata": {
        "id": "2xO74bXzqnrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Important!\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset[0].keys()  # should include 'input_ids', 'attention_mask', 'labels'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "908ee553da4f4007b235e83c73bcda57",
            "23823256d2744975ba751bcf9ac86625",
            "2459bfb2163c416eb8088b22ce81fe34",
            "ac616fc4f65e48ce8bf9cec073b4eb52",
            "596fb7c1d4dd475da283334863c5abc0",
            "c98bd06992a44abfaa593e2d78b72805",
            "4daa76383a2b4965bbb9e2e329dc368f",
            "beb6daa2dc5344678c782602066d66f4",
            "96904e4f0e504fd78ec9fd1a4fe4c7fb",
            "72c4d44d5b274fd087a44b4da4d56abb",
            "6bee63ea34b54c89b707e09ad812de7f"
          ]
        },
        "id": "8HfHgMIoKi7e",
        "outputId": "e9356bcb-ca2e-458e-d960-9cda07028725"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "908ee553da4f4007b235e83c73bcda57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['text', 'input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 7: Training Arguments and train the model"
      ],
      "metadata": {
        "id": "UlHGUjgwqrcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",   # disables wandb\n",
        "    no_cuda=True        # disable GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "H35ICwuKLiS3",
        "outputId": "d9cbd71e-942c-4563-e87d-a9f333fb5dbf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1604: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-71-1286231916.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90/90 15:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.274900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.404900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.239500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.235600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.271300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=90, training_loss=0.5252676486968995, metrics={'train_runtime': 934.8788, 'train_samples_per_second': 0.096, 'train_steps_per_second': 0.096, 'total_flos': 23516282880000.0, 'train_loss': 0.5252676486968995, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vbWMgBg9q1tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "UERNULXGRJ-x"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 9:  Load the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "1Y89UrUsq5AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"./gpt2-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoFUPSzJji2C",
        "outputId": "9a941ab9-beaa-4784-c341-652e759613fe"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint-90']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 10: Load model from checkpoint"
      ],
      "metadata": {
        "id": "AMglf5_rrZbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # still use base tokenizer unless you customized it\n",
        "\n",
        "# Load model from checkpoint\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned/checkpoint-90\")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OJX9HM_pIuD",
        "outputId": "418fefca-4c93-46f5-e2cd-49a020e3e6ae"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 11: Implementation"
      ],
      "metadata": {
        "id": "VKZE5CFurl_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned/checkpoint-90\")\n",
        "\n",
        "# Set pad_token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Encode input text\n",
        "input_text = \" what is attention mechanism,\"\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Generate with attention_mask and pad_token_id\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=100,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlPZFpvGpMmS",
        "outputId": "85efc43d-e63e-45d4-b888-3298f5ac9ff2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " what is attention mechanism, what is the difference between these two approaches, and does this work?\n",
            "Answer: We propose an approach using the network that is an in-memory model-driven model translation, as well as a recurrent convolutional neural network implemented using recurrent recurrent neural networks and recurrent recurrent neural networks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cMz7tmf4pMeC"
      }
    }
  ]
}